<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html>
<head>
  <title>opportunities</title>
  	<link rel="stylesheet" href="../styles/mystyles.css">
  </head>

<body>
<img src="C:\Users\willi\Downloads\0704 - AI in mental health\CTiS-2023_1\images\Artificial_Intelligence,_AI.jpg" width="400" height="250" />

<!-- Site navigation menu -->

<ul class="navbar">
  <li><a href="../index.html">Home page</a>
  <li><a href="topic.html">Technology/Topic</a>
	<li><a href="opportunities.html">Opportunities</a>
	<li><a href="risks.html">Risks</a>
	<li><a href="choices.html">Choices</a>
	<li><a href="ethics.html">Ethical Reflections</a>
	<li><a href="references.html">References</a>
   <li><a href="process.html">Process Support</a>
</ul>

<!-- Main content -->
<h1>Gaps and opportunities</h1>


</ul><p>An AI’s role in mental health such as an automated chatbot can be un-reliant due to issues regarding sympathetic factors. While AI models can analyze the user’s responses, they can often struggle to comprehend the difference of emotions, such as sarcasm or irony. Compared to an actual therapist where they can identify and help resolve your issue based on the sympathy, they may feel for you. This factor can be hard to improve as artificial intelligence lacks emotions a human possesses, however improving an AI's emotional understanding would result in more effective and empathetic interactions in mental health applications.  </p>

<p>Automated chat bots may also lack the ability to understand an individual's background or life experiences that may cause a specific mental health problem such as depression. This would result in the AI to be less effective in helping to resolve the issue. For improvement, an application should be implemented where the AI is able to ask and detect the individual’s background story that led to that mental health problem based on the user's responses. For example, if the AI has the intel that the cause of depression for an individual was due to a breakup, the AI can capitalize on this information and respond accordingly such as quotes which may help uplift the user. This would overall allow for the AI to help resolve the issue much more effectively. </p>

<p>AI in mental health would also need ethical considerations in terms of privacy and data security. When using an AI as a mental health solution such as talking to a chatbot, it may steal or collect an individual’s personal information to improve the program and user experience. However, this may pose risk to users due to the data collected in the system. A terms of service agreement with the user should always be implemented to ensure the safety of the user and address privacy concerns.  </p>

<p>Although AI applications such as a chat bot can help cope with mental health issues it may be ineffective at resolving them. Mental health is a serious problem and requires continuous help and support to resolve it. Rather than relying solely on AI as a standalone solution, it would be a lot more effective if AI were implemented into existing mental healthcare systems.  This would allow for regular support of an individual’s Hauora such as a human therapeutic session during the day and an AI chatbot may be used in the meantime to not only assist in resolving the problem but also track and monitor the individual to ensure that they are safe.  </p>


<!-- Sign and date the page, it's only polite! -->
<address>Made 1 March 2021<br>
  by Tony Clear.</address>
 
<p><em>thanks to W3C for tutorial and adapted code from <a href="https://www.w3.org/Style/Examples/011/firstcss.en.html">Style Examples</a></p></em>
<p><em>also thanks to WDN for HTML and CSS resources and any adapted code snippets from <a href= "https://developer.mozilla.org/en-US/docs/Web">Mozilla Developer Network</a></p></em 
 </html>